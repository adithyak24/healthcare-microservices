spring.application.name=analytics-service
server.port=9002

# Kafka Configuration
spring.kafka.bootstrap-servers=kafka:29092

# Kafka Consumer Configuration
spring.kafka.consumer.group-id=analytics-service-group # Define a consumer group ID
spring.kafka.consumer.auto-offset-reset=earliest

spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer

# Configuration for JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=com.pm.analyticsservice.kafka.dto,com.pm.patientservice.kafka.dto,java.util,java.lang,java.math,java.time
# The above allows deserialization of PatientEventDTO and its fields (like UUID, LocalDate, BigDecimal, LocalDateTime etc.)
# Alternatively, use spring.json.trusted.packages=* for less security but easier setup during dev.

spring.kafka.consumer.properties.spring.json.value.default.type=com.pm.analyticsservice.kafka.dto.PatientEventDTO
spring.kafka.consumer.properties.spring.json.use.type.headers=false
# Tells the JsonDeserializer which type to expect if no type headers are present in the message

# Kafka Producer Configuration (if this service also produces messages)
# spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
# spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

# Logging levels (optional)
logging.level.org.springframework.kafka=INFO
logging.level.com.pm.analyticsservice.kafka=DEBUG
# For more verbose logging from your consumer

spring.datasource.url=jdbc:postgresql://postgres:5432/analytics_db
spring.datasource.username=postgres
spring.datasource.password=root
spring.datasource.driver-class-name=org.postgresql.Driver
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true

# Elasticsearch Configuration
spring.elasticsearch.uris=http://elasticsearch:9200